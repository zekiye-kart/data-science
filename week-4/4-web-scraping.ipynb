{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beautiful Soup docs: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "     ---------------------------------------- 0.0/143.0 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/143.0 kB ? eta -:--:--\n",
      "     ------- ----------------------------- 30.7/143.0 kB 330.3 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/143.0 kB 363.1 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 112.6/143.0 kB 547.6 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 133.1/143.0 kB 605.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 143.0/143.0 kB 531.0 kB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.2 soupsieve-2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 30.7/62.6 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  61.4/62.6 kB 812.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  61.4/62.6 kB 812.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 62.6/62.6 kB 371.2 kB/s eta 0:00:00\n",
      "Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "   ---------------------------------------- 0.0/162.5 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 92.2/162.5 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 153.6/162.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 162.5/162.5 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "   ---------------------------------------- 0.0/100.3 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/100.3 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 100.3/100.3 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.6/61.6 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "   ---------------------------------------- 0.0/104.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/104.6 kB ? eta -:--:--\n",
      "   ---------------------------------------  102.4/104.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 104.6/104.6 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, 'html.parser')  #Beautiful Soup kütüphanesini kullanarak bir HTML belgesini analiz etmek ve içinden veri çekmek için temel bir adım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())  # belgeyi düzenli bir şekilde biçimlendirilmiş olarak ekrana yazdırır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a') #(<a> etiketlerini) bulup ekrana yazdırır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link1 : http://example.com/elsie\n",
      "link2 : http://example.com/lacie\n",
      "link3 : http://example.com/tillie\n"
     ]
    }
   ],
   "source": [
    "# more readable code example:\n",
    "for link in soup.find_all('a'):    #Beautiful Soup'un find_all fonksiyonuyla belgedeki tüm <a> etiketlerini bulur.\n",
    "    id= link.get('id')            #Döngü içinde, her bir <a> etiketinin id ve href özelliklerini link.get('id') ve link.get('href') ile alır.\n",
    "    link = link.get('href')\n",
    "    print(f\"{id} : {link}\")      #f\"{id} : {link}\" ifadesiyle, her bağlantının id ve href bilgilerini birleştirip, ekrana daha okunabilir bir şekilde yazdırır.\n",
    "    \n",
    "    #\"href\", \"Hypertext Reference\"ın kısaltmasıdır ve bir bağlantının hedefini (URL veya başka bir belge) belirtir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text())\n",
    "#BeautifulSoup nesnesi içindeki metin içeriğini çeker. Yani, HTML veya XML belgesindeki tüm metinleri alır ve ekrana yazdırır. \n",
    "# Bu sayede, sadece sayfa metniyle ilgileniyorsanız, diğer etiketlerden arındırılmış metni elde edebilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get URL Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "text = requests.get(url).content.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wiki_soup = BeautifulSoup(text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wiki_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Python (programming language) - Wikipedia</title>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
